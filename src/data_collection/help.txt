src/data_collection: Phase 0 - Raw data gathering from the web

This module implements the first phase of the fact-checking pipeline:
collecting raw evidence from web sources based on a user's claim.

A) web_search.py:
   - Query Generation: Transforms user claim into search queries
   - Web Search API Integration: 
     * SerpApi (Google Search): Use for Google search results
     * Bing Search API: Alternative search engine
   - Returns: Top 20 URLs with metadata (title, snippet, date, source)
   
   Example:
   Claim: "Vietnam is the world's largest coffee exporter"
   → Query: "Vietnam coffee exporter statistics"
   → Returns: [
       {url: "...", title: "...", snippet: "...", date: "...", source: "..."},
       ...
     ]

B) web_scraper.py:
   - Web Scraping: Downloads HTML and extracts clean text
   - Primary Method: trafilatura (fast, accurate)
   - Fallback Method: newspaper3k (robust)
   - Metadata Extraction: title, author, date, description, domain
   
   ⚡ PERFORMANCE OPTIMIZATIONS:
   - Concurrent Execution: ThreadPoolExecutor with max_workers=10
     └─> Parallel scraping reduces 15s → 2-3s (5-7x speedup)
   - Strict Timeouts: timeout=3s per request (fail-fast strategy)
     └─> Prevents slow sites from blocking entire pipeline
   - Text-Only Headers: Accept: text/html (skip images/media)
     └─> Reduces bandwidth by ~20%
   
   For each URL (parallel):
   1) Download HTML content (with 3s timeout)
   2) Extract article text (remove ads, navigation, etc.)
   3) Extract metadata (title, date, author)
   4) Return structured document

C) collector.py:
   - Orchestrator: Combines search + scraping into single pipeline
   - Corpus Creation: Builds collection of 10 documents
   - Data Persistence: Saves corpus to data/raw/ as JSON
   - Summary Statistics: Provides overview of collected data
   
   Complete Pipeline:
   User Claim → Search (10 URLs) → Scrape (10 docs) → Save Corpus
   
   Output Format:
   {
     "claim": "...",
     "search_results": [...],
     "corpus": [
       {
         "url": "...",
         "text": "...",
         "title": "...",
         "date": "...",
         "domain": "...",
         ...
       },
       ...
     ],
     "metadata": {...}
   }

Usage Flow:
-----------
1. Initialize: collector = DataCollector(search_api="serpapi", api_key="...")
2. Collect: corpus = collector.collect_corpus(claim, num_urls=10)
3. Result: 10 documents ready for indexing and retrieval

PERFORMANCE TARGETS:
--------------------
Without Optimization: ~15 seconds (sequential scraping, 10 docs)
With Optimization:    ~2-3 seconds (parallel + timeouts)

Configuration Variables:
- MAX_WORKERS: 10 (concurrent threads)
- TIMEOUT: 3 seconds (per request)
- HEADERS: {'Accept': 'text/html'} (text-only)

Multi-threading Safety:
- ThreadPoolExecutor is safe for concurrent network I/O
- Low memory overhead (~10 threads)
- CPU remains mostly idle during network waits
- No risk to system stability

Next Steps:
-----------
After data collection, the corpus moves to:
- src/retrieval/build_index.py: Index documents for fast retrieval
