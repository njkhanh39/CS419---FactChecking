src/retrieval: Phase 1 - Two-Stage Funnel Architecture

PURPOSE:
--------
This module implements the CORRECT two-stage funnel approach:
- Takes 10 articles from data collection (Phase 0)
- Splits into ~890 sentences
- **Stage 1 (Cheap Filter)**: Build BM25 on ALL 890 ‚Üí Query ‚Üí Top 50 (HIGH RECALL, <0.1s)
- **Stage 2 (Expensive)**: Encode ONLY Top 50 ‚Üí FAISS with 50 embeddings ‚Üí Hybrid Ranking ‚Üí Top 12

‚ö†Ô∏è  CRITICAL: Don't encode all 890 sentences! Use BM25 to filter first (23x speedup)

ARCHITECTURE (CORRECT FUNNEL):
-------------------------------
Input:  data/raw/corpus_*.json (from Phase 0)
        ‚îî‚îÄ> 10 documents with metadata

Process: build_index.py (TWO-STAGE INDEXING)
        1) Split into ~890 sentences
        2) Build BM25 index on ALL 890 sentences (cheap, lexical)
        3) Query BM25 with claim ‚Üí Top 50 candidates
        4) Encode ONLY those 50 sentences (expensive, semantic)
        5) Build FAISS index with 50 embeddings (not 890!)

Output: data/index/
        ‚îú‚îÄ> sentence_store.pkl (ALL ~890 sentences + metadata)
        ‚îú‚îÄ> bm25_index.pkl (BM25 index on ALL 890)
        ‚îú‚îÄ> top50_candidates.pkl (Top 50 from BM25 query)
        ‚îî‚îÄ> faiss_index.bin (ONLY 50 embeddings, not 890!)

Retrieve: retrieval_orchestrator.py
        ‚îî‚îÄ> Stage 1: BM25 ‚Üí Top 50 (already done during indexing)
        ‚îî‚îÄ> Stage 2: Hybrid ranking on 50 ‚Üí Top 12 (high precision)


COMPONENTS:
-----------

A) build_index.py - Index Builder
   
   What it does:
   1) Loads corpus JSON from data/raw/
   2) Splits each document into sentences using regex
   3) Filters sentences (quality: length, duplicates, emojis)
   4) Extracts metadata per sentence (doc_title, url, date, domain)
   5) Builds BM25 index:
      - Tokenizes sentences (lowercase, extract words)
      - Creates inverted index for fast keyword matching
      - Saves to bm25_index.pkl
   6) Builds FAISS index:
      - Encodes sentences to embeddings (all-MiniLM-L6-v2)
      - Normalizes vectors for cosine similarity
      - Saves to faiss_index.bin
   7) Saves sentence_store.pkl (all sentences + metadata)
   
   ‚ö° PERFORMANCE OPTIMIZATIONS:
   
   üö® MOST CRITICAL: FUNNEL ARCHITECTURE (23x speedup!)
   --------------------------------------------------
   ‚ùå WRONG: Encode all 890 sentences ‚Üí 7s (wasteful!)
   ‚úÖ CORRECT: BM25 filter first ‚Üí Encode only top 50 ‚Üí 0.3s
   
   Why this matters:
   - BM25 on 890 sentences: ~0.05s (cheap lexical matching)
   - Encoding 890 sentences: ~7s (expensive neural network)
   - Encoding 50 sentences: ~0.3s (890√∑50 = 17.8x less work)
   - **Total speedup: 23x faster!**
   
   Additional optimizations:
   - Batch Encoding: Process 50 sentences in 2-3 batches (batch_size=16-32)
     ‚îî‚îÄ> Eliminates per-sentence loop overhead
   - Hardware Acceleration: Auto-detect GPU/MPS/CPU
     ‚îî‚îÄ> GPU provides additional 2-3x speedup on encoding
   - Fast Model: all-MiniLM-L6-v2 (384-dim, optimized for speed)
     ‚îî‚îÄ> 4x faster than roberta-large with minimal accuracy loss
   
   PERFORMANCE COMPARISON:
   -----------------------
   ‚ùå Wrong approach (encode all 890):  ~7.0s encoding + 0.05s BM25 = 7.05s
   ‚úÖ Correct approach (funnel):        ~0.05s BM25 + 0.3s encoding = 0.35s
   üìä Speedup: 20x faster!
   
   Configuration Variables:
   - BATCH_SIZE: 32 (batch encoding size)
   - DEVICE: 'cuda'/'mps'/'cpu' (auto-detected)
   - MODEL: 'all-MiniLM-L6-v2' (fast semantic encoder)
   - NUM_SENTENCES: ~200 (from 10 documents)

   Usage:
   ------
   # Option 1: Automatic (uses latest corpus)
   python -m src.retrieval.build_index

   # Option 2: Specify corpus file
   python -m src.retrieval.build_index corpus_Vietnam_coffee_20241127.json

   # Option 3: In code
   from src.retrieval import IndexBuilder
   builder = IndexBuilder()
   builder.build_from_corpus_file('corpus_Vietnam_coffee_20241127.json')

   Output:
   -------
   Loading corpus from: data/raw/corpus_Vietnam_coffee_20241127.json
   Extracted 487 sentences from 20 documents

   1. Saving sentence store...
      Saved 487 sentences to sentence_store.pkl

   2. Building BM25 Index...
      Saved BM25 index to bm25_index.pkl

   3. Building Embedding Index (FAISS)...
      Loading model: all-MiniLM-L6-v2
      Encoding 487 sentences...
      Batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:03<00:00,  4.12it/s]
      Saved FAISS index to faiss_index.bin

   ‚úì All indexes built successfully!
     Location: e:\...\data\index
     Total sentences indexed: 487


B) bm25_retriever.py - BM25 Lexical Retrieval

   What it does:
   - Performs FAST keyword-based retrieval
   - Uses BM25 algorithm (term frequency + inverse document frequency)
   - Goal: HIGH RECALL (capture all potentially relevant sentences)
   - Returns Top 50 sentences by default (Funnel Stage 1)

   Usage:
   ------
   from src.retrieval import BM25Retriever

   retriever = BM25Retriever()
   results = retriever.retrieve(
       query="Vietnam is the second largest coffee exporter",
       top_k=50  # Stage 1: Top 50 for high recall
   )

   # Access results
   for sentence in results:
       print(f"Score: {sentence['score']:.4f}")
       print(f"Text: {sentence['text']}")
       print(f"Source: {sentence['doc_domain']}")

   Example Output:
   ---------------
   [
       {
           'sentence_id': 42,
           'text': 'Vietnam is the world\'s second-largest coffee producer, exporting approximately 1.7 million tons annually.',
           'score': 8.7234,  # BM25 score (higher = better match)
           'doc_id': 5,
           'doc_title': 'Coffee Export Statistics 2023',
           'doc_url': 'https://reuters.com/article/coffee-exports',
           'doc_date': '2023-12-15',
           'doc_domain': 'reuters.com',
           'doc_author': 'John Smith',
           'method': 'bm25'
       },
       {
           'sentence_id': 128,
           'text': 'After Brazil, Vietnam ranks as the leading exporter of coffee beans globally.',
           'score': 7.9432,
           'doc_id': 8,
           'doc_title': 'Global Coffee Trade Report',
           'doc_url': 'https://tradereport.com/coffee',
           'doc_date': '2023-11-20',
           'doc_domain': 'tradereport.com',
           'doc_author': 'Jane Doe',
           'method': 'bm25'
       },
       ... (48 more sentences)
   ]


C) embed_retriever.py - Semantic Embedding Retrieval

   What it does:
   - Performs SEMANTIC similarity search using embeddings
   - Uses sentence-transformers (all-MiniLM-L6-v2) + FAISS
   - Captures semantic meaning (not just keywords)
   - Returns sentences with cosine similarity scores (0-1)
   - Can be used standalone OR for computing semantic scores in Phase 2

   Usage:
   ------
   from src.retrieval import EmbeddingRetriever

   retriever = EmbeddingRetriever()
   
   # Option 1: Retrieve top-k sentences
   results = retriever.retrieve(
       query="Vietnam coffee export ranking",
       top_k=10
   )

   # Option 2: Compute similarity for specific sentences
   sentences = ["Vietnam exports coffee.", "Brazil produces sugar."]
   similarities = retriever.compute_similarity(
       query="Vietnam coffee",
       sentences=sentences
   )
   # Returns: array([0.85, 0.23])

   Example Output:
   ---------------
   [
       {
           'sentence_id': 42,
           'text': 'Vietnam is the world\'s second-largest coffee producer...',
           'score': 0.8756,  # Cosine similarity (0-1, higher = more similar)
           'doc_id': 5,
           'doc_title': 'Coffee Export Statistics 2023',
           'doc_url': 'https://reuters.com/article/coffee-exports',
           'doc_date': '2023-12-15',
           'doc_domain': 'reuters.com',
           'doc_author': 'John Smith',
           'method': 'embedding'
       },
       {
           'sentence_id': 215,
           'text': 'The Southeast Asian nation shipped 27 million bags...',
           'score': 0.8234,
           'doc_id': 12,
           'doc_title': 'Vietnam Coffee Industry Overview',
           'doc_url': 'https://coffeenews.com/vietnam',
           'doc_date': '2023-10-05',
           'doc_domain': 'coffeenews.com',
           'doc_author': 'Mike Johnson',
           'method': 'embedding'
       },
       ... (8 more sentences)
   ]


TYPICAL WORKFLOW (PER CLAIM):
------------------------------
Each claim requires fresh evidence collection from the web!

Complete Pipeline for ONE Claim:
---------------------------------

Step 0: Collect Fresh Data (Phase 0 - Data Collection)
-------------------------------------------------------
from src.data_collection import DataCollector

collector = DataCollector(search_api="serpapi")
claim = "Vietnam is the second largest coffee exporter"

# Search web and scrape 20 fresh articles for THIS claim
corpus = collector.collect_corpus(claim, num_urls=20)
# Saves to: data/raw/corpus_Vietnam_is_the_second_20241127_143022.json


Step 1: Build Indexes (Phase 1 - Index Building)
-------------------------------------------------
from src.retrieval import IndexBuilder

builder = IndexBuilder()
# Build indexes from the fresh corpus we just collected
builder.build_from_corpus_file('corpus_Vietnam_is_the_second_20241127_143022.json')
# This creates: sentence_store.pkl, bm25_index.pkl, faiss_index.bin
# (~500 sentences from 20 articles)


Step 2: Retrieve Top 50 Sentences (Stage 1 - Fast Filtering)
-------------------------------------------------------------
from src.retrieval import BM25Retriever

retriever = BM25Retriever()
top_50_sentences = retriever.retrieve(claim, top_k=50)

print(f"Retrieved {len(top_50_sentences)} candidate sentences")
# Output: Retrieved 50 candidate sentences


Step 3: Pass to Phase 2 (Sentence Ranking)
-------------------------------------------
# The top_50_sentences are now ready for Phase 2
# Phase 2 will:
# 1) Compute semantic scores (using EmbeddingRetriever)
# 2) Reuse BM25 scores (already in results)
# 3) Compute metadata scores
# 4) Combine: 0.5*semantic + 0.3*lexical + 0.2*metadata
# 5) Select Top 10 sentences

from src.sentence_ranker import SentenceRanker
ranker = SentenceRanker()
top_10_sentences = ranker.rank(claim, top_50_sentences, top_k=10)


IMPORTANT NOTES:
----------------
‚úì Each NEW claim needs its own data collection (fresh web search & scraping)
‚úì You CANNOT reuse old corpus for different claims
‚úì Example: Coffee export corpus won't help verify climate change claims

Why build indexes at all (instead of just searching in memory)?
- BM25 needs inverted index structure for efficiency
- FAISS needs normalized vectors for fast similarity search
- Even for 500 sentences, indexing makes retrieval 100x faster
- Building indexes takes 5-10 seconds, but retrieval takes <100ms


PERFORMANCE:
------------
- Index Building: ~5-10 seconds for 500 sentences
- BM25 Retrieval: <100ms for top-50 from 500 sentences
- Embedding Retrieval: ~200ms for top-10 from 500 sentences

TIPS:
-----
1. Each NEW claim requires fresh data collection (web search + scraping)
2. Build indexes immediately after data collection for each claim
3. Use BM25 for Stage 1 (fast, high recall) - get top-50
4. Use Embeddings for Stage 2 semantic scoring (slower, high precision) - refine to top-10
5. For the same claim with same corpus, you can reuse indexes (testing/debugging)
6. In production: collect ‚Üí index ‚Üí retrieve ‚Üí rank ‚Üí NLI (full pipeline per claim)

TROUBLESHOOTING:
----------------
Error: "BM25 index not found"
‚Üí Solution: Run build_index.py first

Error: "Sentence store not found"
‚Üí Solution: Run build_index.py to create sentence_store.pkl

Error: "No corpus files found"
‚Üí Solution: Run data collection (Phase 0) first to create corpus JSON

Low BM25 scores (all <1.0):
‚Üí Normal: BM25 scores vary by query, focus on relative ranking

Low cosine similarity (all <0.3):
‚Üí Possible: Corpus doesn't contain relevant information for the claim